{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "Whereas pictures and audo is inherently mathematical in nature, sentances have no mathematical analog. This requires text data to be embedded in a mathematical framework when used in machine learning. One hot encoding can work but does not retain relations between words or what the words themselves mean. NLP (natural language processing mains to provide a possible solution to this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing text data to embed\n",
    "\n",
    "with open('../Data/49960') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "contents = [x for x in contents.split(' ') if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(852, 852)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer() #initialize the vectorizer\n",
    "\n",
    "X = vectorizer.fit_transform(contents) #fit the vectorizer to the data and generate a matrix\n",
    "Xc = (X.T @ X).todense() #required since the form returned by the above is a compressed sparsed matrix\n",
    "Xc.shape #We see that it ranks how many times each word occured with every other unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 components explain 90.99838080214158 of the variance.\n",
      "100 components explain 93.96027055575762 of the variance.\n",
      "150 components explain 95.57991442437799 of the variance.\n",
      "200 components explain 96.51960897512026 of the variance.\n",
      "250 components explain 97.45046956936194 of the variance.\n",
      "300 components explain 97.83409350244322 of the variance.\n",
      "350 components explain 98.06901714012851 of the variance.\n",
      "400 components explain 98.3039407778138 of the variance.\n",
      "450 components explain 98.53886441549909 of the variance.\n",
      "500 components explain 98.77378805318439 of the variance.\n",
      "550 components explain 99.00871169086967 of the variance.\n",
      "600 components explain 99.24363532855497 of the variance.\n",
      "650 components explain 99.47855896624026 of the variance.\n",
      "700 components explain 99.71348260392556 of the variance.\n",
      "750 components explain 99.93584081502573 of the variance.\n"
     ]
    }
   ],
   "source": [
    "#notice that this can be thought of as 852 instances with 852 features.\n",
    "#Checking with PCA allows us to reduce the dimension to improve computation times.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_ini = PCA()\n",
    "exp_var = pca_ini.fit(np.asarray(Xc)).explained_variance_ratio_\n",
    "\n",
    "for n in range(50,800,50):\n",
    "    print(f'{n} components explain {sum(exp_var[:n])*100} of the variance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(852, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the feature names are given by\n",
    "\n",
    "wordlabels = vectorizer.get_feature_names_out()\n",
    "\n",
    "#using 100 compents then we can reduce the dimension of the co-oorence matrix to have 100 features.\n",
    "\n",
    "pca = PCA(n_components= 100)\n",
    "embeddings = pca.fit_transform(np.asarray(Xc))\n",
    "embeddings.shape\n",
    "#The words are now embedded in a mathematical form. From this we can see which words are closely associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short :   0.0\n",
      "compromise :   0.19154606252369427\n",
      "less :   0.9321305669073944\n",
      "formalistic :   0.9321305669073944\n",
      "273 :   0.944492651476401\n"
     ]
    }
   ],
   "source": [
    "# nearest neighbors\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Euclidean distance on normalized vectors is cosine distance\n",
    "embeddings = normalize(embeddings)\n",
    "tree = KDTree(embeddings)\n",
    "\n",
    "evalWord = 'short'\n",
    "k = 5 # five closest words\n",
    "\n",
    "dist, ind = tree.query([embeddings[list(wordlabels).index(evalWord)]], k=k)\n",
    "\n",
    "for i in range(k):\n",
    "    print(wordlabels[ind[0][i]], \":  \", dist[0][i])\n",
    "\n",
    "#Note the predictions are only as good as the data you feed it.\n",
    "#This is what embedding is but modern methods are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 5, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17eab0df9e3307548a5c6f41d73e01b4dc6a359441bcee24f0d97b016c3af62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
