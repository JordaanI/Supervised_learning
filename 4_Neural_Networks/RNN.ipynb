{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(list_of_words, sequence_length):\n",
    "    res = []\n",
    "    for i in range(len(list_of_words)-sequence_length):\n",
    "        res.append((list_of_words[i:i+sequence_length],list_of_words[i+sequence_length]))\n",
    "    return res\n",
    "\n",
    "def splitter(database, fraction, num_words):\n",
    "    \n",
    "    training = [x for x in database['tokens']]\n",
    "    labels = [x for x in database['label_tokens']]\n",
    "\n",
    "    X_train = np.array(training[:int(fraction*len(training))])\n",
    "    X_test = np.array(training[int(fraction*len(training)):])\n",
    "\n",
    "    y_train_base = np.array(labels)[:int(fraction*len(labels))]\n",
    "    y_test_base = np.array(labels)[int(fraction*len(labels)):]\n",
    "\n",
    "    y_train = np.zeros((len(y_train_base), num_words), dtype=np.int8)\n",
    "    y_test = np.zeros((len(y_test_base), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(y_train_base):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(y_test_base):\n",
    "        y_test[example_index, word_index] = 1\n",
    "\n",
    "    print(f'The training sequence shape is {X_train.shape}, the training label shape is {y_train.shape}')\n",
    "    print(f'The test sequence shape is {X_test.shape}, the test label shape is  {y_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600218</th>\n",
       "      <td>[shown, and, explained, and, each, was, wonder...</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263459</th>\n",
       "      <td>[the, stranger, said, quietly, i, saw, and, he...</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63108</th>\n",
       "      <td>[bigger, than, a, large, sewerpipe, and, found...</td>\n",
       "      <td>wormed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684421</th>\n",
       "      <td>[whispered, with, enthusiasm, and, belief, of,...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501893</th>\n",
       "      <td>[the, limited, personality, he, had, been, so,...</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180928</th>\n",
       "      <td>[lot, welcome, the, lot, and, thushe, could, n...</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489878</th>\n",
       "      <td>[the, practical, results, obtainable, by, soun...</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589941</th>\n",
       "      <td>[for, one, another, it, was, the, mating, of, ...</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640624</th>\n",
       "      <td>[some, ethereal, bird, of, fire, rising, into,...</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167655</th>\n",
       "      <td>[a, voice, that, held, a, note, of, softness, ...</td>\n",
       "      <td>devonham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sequence     label\n",
       "600218  [shown, and, explained, and, each, was, wonder...         a\n",
       "263459  [the, stranger, said, quietly, i, saw, and, he...      made\n",
       "63108   [bigger, than, a, large, sewerpipe, and, found...    wormed\n",
       "684421  [whispered, with, enthusiasm, and, belief, of,...        he\n",
       "501893  [the, limited, personality, he, had, been, so,...    result\n",
       "180928  [lot, welcome, the, lot, and, thushe, could, n...        in\n",
       "489878  [the, practical, results, obtainable, by, soun...       are\n",
       "589941  [for, one, another, it, was, the, mating, of, ...        in\n",
       "640624  [some, ethereal, bird, of, fire, rising, into,...     could\n",
       "167655  [a, voice, that, held, a, note, of, softness, ...  devonham"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books= []\n",
    "\n",
    "\n",
    "delims = ' |\\n|\\ufeff|_'\n",
    "punctuation = '.|,|;|:'\n",
    "wrapping = r'\\--(.--?)\\--'\n",
    "sequence_length = 50\n",
    "\n",
    "for i in range(1,11):\n",
    "    with open(f'../Data/ABT/{i}.txt', mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        contents = file.read().lower()\n",
    "        contents = re.sub(r'[^\\w\\s]', '', contents)\n",
    "        contents = re.split(delims, contents)\n",
    "        books.append([x for x in contents if x != ''])\n",
    "\n",
    "for ind, book in enumerate(books):\n",
    "    books[ind] = generate_sequence(book, sequence_length=sequence_length)\n",
    "\n",
    "master = []\n",
    "\n",
    "for book in books:\n",
    "    for sequence in book:\n",
    "        master.append(sequence)\n",
    "\n",
    "master_frame = pd.DataFrame(master, columns=['sequence','label'])\n",
    "master_frame.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332083</th>\n",
       "      <td>[inner, catastrophe, he, dreaded, while, desir...</td>\n",
       "      <td>found</td>\n",
       "      <td>[380, 3581, 6, 2032, 130, 6006, 9, 15, 22, 54,...</td>\n",
       "      <td>[169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332598</th>\n",
       "      <td>[cause, unknown, was, the, scientific, verdict...</td>\n",
       "      <td>of</td>\n",
       "      <td>[747, 857, 8, 1, 1954, 6595, 2, 38, 1730, 8210...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551636</th>\n",
       "      <td>[came, wriggling, along, like, a, great, cat, ...</td>\n",
       "      <td>but</td>\n",
       "      <td>[66, 8882, 299, 53, 4, 104, 751, 5, 9688, 28, ...</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152322</th>\n",
       "      <td>[in, the, private, sittingroom, levallon, seat...</td>\n",
       "      <td>been</td>\n",
       "      <td>[7, 1, 1259, 2920, 328, 2502, 7, 38, 1881, 35,...</td>\n",
       "      <td>[71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438894</th>\n",
       "      <td>[me, with, a, vague, unrest, unsettled, seekin...</td>\n",
       "      <td>that</td>\n",
       "      <td>[28, 13, 4, 1349, 4423, 7752, 1721, 5, 2710, 7...</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sequence  label  \\\n",
       "332083  [inner, catastrophe, he, dreaded, while, desir...  found   \n",
       "332598  [cause, unknown, was, the, scientific, verdict...     of   \n",
       "551636  [came, wriggling, along, like, a, great, cat, ...    but   \n",
       "152322  [in, the, private, sittingroom, levallon, seat...   been   \n",
       "438894  [me, with, a, vague, unrest, unsettled, seekin...   that   \n",
       "\n",
       "                                                   tokens label_tokens  \n",
       "332083  [380, 3581, 6, 2032, 130, 6006, 9, 15, 22, 54,...        [169]  \n",
       "332598  [747, 857, 8, 1, 1954, 6595, 2, 38, 1730, 8210...          [3]  \n",
       "551636  [66, 8882, 299, 53, 4, 104, 751, 5, 9688, 28, ...         [20]  \n",
       "152322  [7, 1, 1259, 2920, 328, 2502, 7, 38, 1881, 35,...         [71]  \n",
       "438894  [28, 13, 4, 1349, 4423, 7752, 1721, 5, 2710, 7...         [11]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(master_frame['sequence'])\n",
    "master_frame['tokens'] = tokenizer.texts_to_sequences(master_frame['sequence'])\n",
    "master_frame['label_tokens'] = tokenizer.texts_to_sequences(master_frame['label'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "master_frame.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 300)           7485600   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50, 64)            93440     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50, 64)            33024     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               38700     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24952)             7510552   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,227,364\n",
      "Trainable params: 15,227,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dropout, Bidirectional, Dense\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                filepath='../Data/weights_RNN.h5',\n",
    "                                                save_weights_only=True,\n",
    "                                                monitor='val_accuracy',\n",
    "                                                mode='max',\n",
    "                                                save_best_only=True)\n",
    "\n",
    "LSTM_cells = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 300, input_length=50))\n",
    "model.add(LSTM(LSTM_cells,return_sequences=True,dropout=0.1))\n",
    "model.add(LSTM(LSTM_cells,return_sequences=True,dropout=0.1))\n",
    "model.add(Bidirectional(LSTM(LSTM_cells,return_sequences=False,dropout=0.1)))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.save('../Data/RNN_model.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training sequence shape is (129563, 50), the training label shape is (129563, 24952)\n",
      "The test sequence shape is (14396, 50), the test label shape is  (14396, 24952)\n",
      "Epoch 1/40\n",
      "40/40 [==============================] - 15s 121ms/step - loss: 8.7717 - accuracy: 0.0598 - val_loss: 7.9453 - val_accuracy: 0.0469\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 7.4245 - accuracy: 0.0590 - val_loss: 7.7670 - val_accuracy: 0.0469\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 7.2996 - accuracy: 0.0551 - val_loss: 7.6642 - val_accuracy: 0.0469\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 7.2237 - accuracy: 0.0617 - val_loss: 7.6182 - val_accuracy: 0.0469\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 7.0598 - accuracy: 0.0645 - val_loss: 7.5614 - val_accuracy: 0.0469\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 7.1903 - accuracy: 0.0520 - val_loss: 7.6230 - val_accuracy: 0.0469\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 7.0670 - accuracy: 0.0586 - val_loss: 7.5135 - val_accuracy: 0.0469\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 7.0212 - accuracy: 0.0676 - val_loss: 7.5062 - val_accuracy: 0.0469\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9397 - accuracy: 0.0629 - val_loss: 7.4427 - val_accuracy: 0.0469\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9894 - accuracy: 0.0582 - val_loss: 7.4679 - val_accuracy: 0.0469\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.9174 - accuracy: 0.0570 - val_loss: 7.4085 - val_accuracy: 0.0469\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9123 - accuracy: 0.0633 - val_loss: 7.4177 - val_accuracy: 0.0469\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9686 - accuracy: 0.0609 - val_loss: 7.4105 - val_accuracy: 0.0469\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9053 - accuracy: 0.0660 - val_loss: 7.4732 - val_accuracy: 0.0469\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.8831 - accuracy: 0.0578 - val_loss: 7.4532 - val_accuracy: 0.0469\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.9122 - accuracy: 0.0578 - val_loss: 7.4247 - val_accuracy: 0.0469\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.8813 - accuracy: 0.0578 - val_loss: 7.4050 - val_accuracy: 0.0469\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.8482 - accuracy: 0.0562 - val_loss: 7.3797 - val_accuracy: 0.0469\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.7722 - accuracy: 0.0590 - val_loss: 7.3585 - val_accuracy: 0.0469\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.8839 - accuracy: 0.0582 - val_loss: 7.3488 - val_accuracy: 0.0469\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 6.7849 - accuracy: 0.0680 - val_loss: 7.4619 - val_accuracy: 0.0469\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.8647 - accuracy: 0.0555 - val_loss: 7.3808 - val_accuracy: 0.0469\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.8481 - accuracy: 0.0598 - val_loss: 7.3445 - val_accuracy: 0.0469\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.9124 - accuracy: 0.0609 - val_loss: 7.3557 - val_accuracy: 0.0469\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.9340 - accuracy: 0.0469 - val_loss: 7.3763 - val_accuracy: 0.0469\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.9011 - accuracy: 0.0609 - val_loss: 7.3240 - val_accuracy: 0.0469\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.8305 - accuracy: 0.0582 - val_loss: 7.3317 - val_accuracy: 0.0469\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7807 - accuracy: 0.0645 - val_loss: 7.3420 - val_accuracy: 0.0469\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7984 - accuracy: 0.0586 - val_loss: 7.3542 - val_accuracy: 0.0469\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7654 - accuracy: 0.0594 - val_loss: 7.3212 - val_accuracy: 0.0469\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 6.7736 - accuracy: 0.0691 - val_loss: 7.3803 - val_accuracy: 0.0469\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 6.8629 - accuracy: 0.0574 - val_loss: 7.3106 - val_accuracy: 0.0469\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.7884 - accuracy: 0.0621 - val_loss: 7.3754 - val_accuracy: 0.0469\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7741 - accuracy: 0.0598 - val_loss: 7.3227 - val_accuracy: 0.0469\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.8121 - accuracy: 0.0598 - val_loss: 7.3264 - val_accuracy: 0.0469\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7854 - accuracy: 0.0625 - val_loss: 7.2937 - val_accuracy: 0.0469\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7456 - accuracy: 0.0652 - val_loss: 7.2899 - val_accuracy: 0.0469\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 6.8220 - accuracy: 0.0582 - val_loss: 7.2986 - val_accuracy: 0.0469\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.6310 - accuracy: 0.0594 - val_loss: 7.3038 - val_accuracy: 0.0469\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 6.7587 - accuracy: 0.0594 - val_loss: 7.3117 - val_accuracy: 0.0469\n",
      "The training sequence shape is (129563, 50), the training label shape is (129563, 24952)\n",
      "The test sequence shape is (14396, 50), the test label shape is  (14396, 24952)\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11012/1630312293.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/RNN_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         model.fit(X_train, y_train, batch_size=64,\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gaming\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gaming\\Documents\\Concordia Bootcamp\\Python\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = 40\n",
    "epochs = 40\n",
    "\n",
    "step = 0.2\n",
    "cut_list = step * np.arange(5)\n",
    "\n",
    "for start, end in zip(cut_list, cut_list+step):\n",
    "    \n",
    "        temp_frame = master_frame.iloc[int(start*len(master_frame)):int(end*len(master_frame))+1]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = splitter(temp_frame,0.9, num_words)\n",
    "\n",
    "        model = load_model('../Data/RNN_model.h5')\n",
    "\n",
    "        model.fit(X_train, y_train, batch_size=64,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=epochs,\n",
    "                callbacks=callback,\n",
    "                validation_data=(X_test,y_test),\n",
    "                validation_steps=10)\n",
    "\n",
    "        model.save('../Data/RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17eab0df9e3307548a5c6f41d73e01b4dc6a359441bcee24f0d97b016c3af62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
