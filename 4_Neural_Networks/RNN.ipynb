{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(list_of_words, sequence_length):\n",
    "    res = []\n",
    "    for i in range(len(list_of_words)-sequence_length):\n",
    "        res.append((list_of_words[i:i+sequence_length],list_of_words[i+sequence_length]))\n",
    "    return res\n",
    "\n",
    "def splitter(database, fraction):\n",
    "    \n",
    "    training = database['tokens']\n",
    "    labels = database['label_tokens']\n",
    "\n",
    "    X_train = np.array(training[:int(0.75*len(training))])\n",
    "    X_test = np.array(training[int(0.75*len(training)):])\n",
    "\n",
    "    y_train_base = np.array(labels)[:int(0.75*len(labels))]\n",
    "    y_test_base = np.array(labels)[int(0.75*len(labels)):]\n",
    "\n",
    "    y_train = np.zeros((len(y_train_base), num_words), dtype=np.int8)\n",
    "    y_test = np.zeros((len(y_test_base), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(y_train_base):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(y_test_base):\n",
    "        y_test[example_index, word_index] = 1\n",
    "\n",
    "    print(f'The training sequence shape is {X_train.shape}, the training label shape is {y_train.shape}')\n",
    "    print(f'The test sequence shape is {X_test.shape}, the test label shape is  {y_test.shape}')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books= []\n",
    "\n",
    "\n",
    "delims = ' |\\n|\\ufeff|_'\n",
    "punctuation = '.|,|;|:'\n",
    "wrapping = r'\\--(.--?)\\--'\n",
    "sequence_length = 50\n",
    "\n",
    "for i in range(1,11):\n",
    "    with open(f'../Data/ABT/{i}.txt', mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        contents = file.read().lower()\n",
    "        contents = re.sub(r'[^\\w\\s]', '', contents)\n",
    "        contents = re.split(delims, contents)\n",
    "        books.append([x for x in contents if x != ''])\n",
    "\n",
    "for ind, book in enumerate(books):\n",
    "    books[ind] = generate_sequence(book, sequence_length=sequence_length)\n",
    "\n",
    "master = []\n",
    "\n",
    "for book in books:\n",
    "    for sequence in book:\n",
    "        master.append(sequence)\n",
    "\n",
    "master_frame = pd.DataFrame(master, columns=['sequence','label'])\n",
    "master_frame.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(master_frame['sequence'])\n",
    "master_frame['tokens'] = tokenizer.texts_to_sequences(master_frame['sequence'])\n",
    "master_frame['label_tokens'] = tokenizer.texts_to_sequences(master_frame['label'])\n",
    "\n",
    "word_lexicon = tokenizer.word_index\n",
    "word_index = tokenizer.index_word\n",
    "num_words = len(word_lexicon) + 1\n",
    "word_counts = tokenizer.word_counts\n",
    "sorted_counts = dict(sorted(dict(word_counts).items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitter(master_frame,0.9)\n",
    "\n",
    "master_frame.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dropout, Bidirectional, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "LSTM_cells = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 300, input_length=50))\n",
    "model.add(LSTM(LSTM_cells,return_sequences=True,dropout=0.1))\n",
    "model.add(LSTM(LSTM_cells,return_sequences=True,dropout=0.1))\n",
    "model.add(Bidirectional(LSTM(LSTM_cells,return_sequences=False,dropout=0.1)))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = 40\n",
    "epochs = 40\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                filepath='../Data/weights_RNN.h5',\n",
    "                                                save_weights_only=True,\n",
    "                                                monitor='val_accuracy',\n",
    "                                                mode='max',\n",
    "                                                save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=epochs,\n",
    "          callbacks=callback,\n",
    "          validation_data=(X_test,y_test),\n",
    "          validation_steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17eab0df9e3307548a5c6f41d73e01b4dc6a359441bcee24f0d97b016c3af62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
